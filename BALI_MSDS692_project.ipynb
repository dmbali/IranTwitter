{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bali MSDS 692 Project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text and Sentiment Analysis\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objectives"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data\n",
    "I am going to use Twitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import twitter as tw\n",
    "import pandas as pd\n",
    "import tweepy as twe\n",
    "import json\n",
    "import pandas as pd\n",
    "import csv\n",
    "import re #regular expression\n",
    "from textblob import TextBlob\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "yVXEZTyzWOjo6Cm1fpi7BcTZC\n"
     ]
    }
   ],
   "source": [
    "c_key = os.environ.get('c_key')\n",
    "print(c_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Blue Cheese Salad x Nudy\n"
     ]
    }
   ],
   "source": [
    "#This is setting up the authentication keys and seeing if I am hitting twitter.\n",
    "c_sec = os.environ.get('c_sec')\n",
    "atk = os.environ.get('atk')\n",
    "ats = os.environ.get('ats')\n",
    "\n",
    "auth = twe.OAuthHandler(c_key, c_sec)\n",
    "auth.set_access_token(atk, ats)\n",
    "\n",
    "api = twe.API(auth)\n",
    "\n",
    "results = api.search(q='cheese', count=100)\n",
    "\n",
    "print(results[1].text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up the new searchtweets authentication"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://developer.twitter.com/en/docs/tweets/search/overview/premium\n",
    "my environ: https://developer.twitter.com/en/account/environments\n",
    "my app:  https://developer.twitter.com/en/apps/17160618\n",
    "full-archive endpoint: https://api.twitter.com/1.1/tweets/search/fullarchive/USIRCrisis.json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consumer API Keys\n",
    "api key: tHbXfB7snrkmEBNxtfHemZ8OX\n",
    "secret: JWDDybbrxvhyH7Pf9cIswbHWqDFWlYVo0RYs1Dl1aEVRReP4bb\n",
    "\n",
    "Access Token\n",
    "token:  468498444-pwjvK2OJaNCSPkrf4VnPtDgQOrqOnX4jc7p53jjE\n",
    "secret: wwIoHBh1Sm7gVK8DRzcJ1zMuNMqQAQNx9lwgyZF4ehGzD\n",
    "\n",
    "export SEARCHTWEETS_ACCOUNT_TYPE= 'premium'\n",
    "export SEARCHTWEETS_ENDPOINT= 'https://api.twitter.com/1.1/tweets/search/fullarchive/USIRCrisis.json'\n",
    "export SEARCHTWEETS_BEARER_TOKEN= 'tHbXfB7snrkmEBNxtfHemZ8OX:JWDDybbrxvhyH7Pf9cIswbHWqDFWlYVo0RYs1Dl1aEVRReP4bb\"\n",
    "\n",
    "Access Token\n",
    "export SEARCHTWEETS_CONSUMER_KEY= '\n",
    "export SEARCHTWEETS_CONSUMER_SECRET= '\n",
    "\n",
    "Bearer token: https://developer.twitter.com/en/docs/basics/authentication/oauth-2-0\n",
    "App only auth: https://developer.twitter.com/en/docs/basics/authentication/oauth-2-0/application-only\n",
    "to create bearer token: Concatenate the encoded consumer key, a colon character \":\", and the encoded consumer secret into a single string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#https://twitterdev.github.io/search-tweets-python/\n",
    "from searchtweets import load_credentials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'os' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-667f0e3041f6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menviron\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"SEARCHTWEETS_USERNAME\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"USIRCrisis\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menviron\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"SEARCHTWEETS_ENDPOINT\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"https://api.twitter.com/1.1/tweets/search/fullarchive/USIRCrisis.json\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menviron\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'SEARCHTWEETS_ACCOUNT_TYPE'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0;34m'premium'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menviron\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'SEARCHTWEETS_BEARER_TOKEN'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0;34m'tHbXfB7snrkmEBNxtfHemZ8OX:JWDDybbrxvhyH7Pf9cIswbHWqDFWlYVo0RYs1Dl1aEVRReP4bb'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'os' is not defined"
     ]
    }
   ],
   "source": [
    "os.environ[\"SEARCHTWEETS_USERNAME\"] = \"USIRCrisis\"\n",
    "os.environ[\"SEARCHTWEETS_ENDPOINT\"] = \"https://api.twitter.com/1.1/tweets/search/fullarchive/USIRCrisis.json\"\n",
    "os.environ['SEARCHTWEETS_ACCOUNT_TYPE']= 'premium'\n",
    "os.environ['SEARCHTWEETS_BEARER_TOKEN']= 'tHbXfB7snrkmEBNxtfHemZ8OX:JWDDybbrxvhyH7Pf9cIswbHWqDFWlYVo0RYs1Dl1aEVRReP4bb'\n",
    "\n",
    "load_credentials(filename=\"nothing_here.yaml\", yaml_key=\"no_key_here\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "premium_search_args = load_credentials(\"~/.twitter_keys.yaml\",\n",
    "                                       yaml_key=\"search_tweets_premium\",\n",
    "                                       env_overwrite=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://github.com/twitterdev/search-tweets-python/issues/64\n",
    "https://developer.twitter.com/en/docs/api-reference-index\n",
    "https://tweepy.readthedocs.io/en/latest/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ushas/anaconda3/lib/python3.7/site-packages/searchtweets/credentials.py:34: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.\n",
      "  search_creds = yaml.load(f)[yaml_key]\n"
     ]
    }
   ],
   "source": [
    "premium_search_args = load_credentials(\"~/MSDS692/twitter_keys.yaml\",\n",
    "                                       yaml_key=\"search_tweets_fullarchive_dev\",\n",
    "                                       env_overwrite=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from searchtweets import ResultStream, gen_rule_payload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"query\": \"beyonce\", \"maxResults\": 100}\n"
     ]
    }
   ],
   "source": [
    "#testing\n",
    "rule = gen_rule_payload(\"beyonce\", results_per_call=100) # testing with a sandbox account\n",
    "print(rule)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_words = 'iran OR #iranprotests2020 OR irianprotesters OR #iran OR #tehran OR #khamenei or #QassemSoleimani'\n",
    "date_since = \"2019-11-01\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets = api.search(q=search_words, count=2000, lang='en', since=date_since)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [[tweet.text] for tweet in tweets]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['In Iran, the supreme leader-IRGC partnership is very effective. https://t.co/NvED8o6dSf\\n\\n#MiddleEast ðŸŒ #MidEastâ€¦ https://t.co/X42DXpvA3V'], [\"Perhaps the reason,I assume,the #Chinese Govt or the World wasn't promptly taking steps to condemn theâ€¦ https://t.co/ixEc5bdOm0\"], ['RT @GIS_Reports: Economic reform would hurt Iranâ€™s most powerful groups. https://t.co/3edA0Ii3uy\\n\\n#MiddleEast ðŸŒ #MidEast #Iran ðŸ‡®ðŸ‡· #Tehran #â€¦'], ['Economic reform would hurt Iranâ€™s most powerful groups. https://t.co/3edA0Ii3uy\\n\\n#MiddleEast ðŸŒ #MidEast #Iran ðŸ‡®ðŸ‡·â€¦ https://t.co/fjsLfcv7eO']]\n"
     ]
    }
   ],
   "source": [
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(xmas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xmas_s = ' '.join([str(elem) for elem in xmas]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xmas_s[0:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Methods and Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparing the environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "import wordcloud\n",
    "import spacy #conda install -c conda-forge spacy-model-en_core_web_lg\n",
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "from spacy.attrs import ORTH\n",
    "nlp = spacy.load('en_core_web_lg')\n",
    "import string\n",
    "import collections\n",
    "from collections import Counter\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics import silhouette_score\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import pandas as pd\n",
    "import csv\n",
    "import re #regular expression\n",
    "from textblob import TextBlob\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import TweetTokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#HappyEmoticons\n",
    "emoticons_happy = set([\n",
    "    ':-)', ':)', ';)', ':o)', ':]', ':3', ':c)', ':>', '=]', '8)', '=)', ':}',\n",
    "    ':^)', ':-D', ':D', '8-D', '8D', 'x-D', 'xD', 'X-D', 'XD', '=-D', '=D',\n",
    "    '=-3', '=3', ':-))', \":'-)\", \":')\", ':*', ':^*', '>:P', ':-P', ':P', 'X-P',\n",
    "    'x-p', 'xp', 'XP', ':-p', ':p', '=p', ':-b', ':b', '>:)', '>;)', '>:-)',\n",
    "    '<3'\n",
    "    ])\n",
    "# Sad Emoticons\n",
    "emoticons_sad = set([\n",
    "    ':L', ':-/', '>:/', ':S', '>:[', ':@', ':-(', ':[', ':-||', '=L', ':<',\n",
    "    ':-[', ':-<', '=\\\\', '=/', '>:(', ':(', '>.<', \":'-(\", \":'(\", ':\\\\', ':-c',\n",
    "    ':c', ':{', '>:\\\\', ';('\n",
    "    ])\n",
    "#Emoji patterns\n",
    "emoji_pattern = re.compile(\"[\"\n",
    "         u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "         u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "         u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "         u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "         u\"\\U00002702-\\U000027B0\"\n",
    "         u\"\\U000024C2-\\U0001F251\"\n",
    "         \"]+\", flags=re.UNICODE)\n",
    "emoticons = emoticons_happy.union(emoticons_sad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def clean_tweets(tweet):\n",
    "    tokenZR = TweetTokenizer()\n",
    "    stop_words = set(stopwords.words('english') and ['RT', '...', 'ðŸ“£', 'ðŸŽ„', 'Merry Christmas', \"MERRY CHRISTMAS\"])\n",
    "    word_tokens = tokenZR.tokenize(tweet)\n",
    "#after tweepy preprocessing the colon symbol left remain after      #removing mentions\n",
    "    tweet = re.sub(r':', '', tweet)\n",
    "    tweet = re.sub(r'â€šÃ„Â¶', '', tweet)\n",
    "#replace consecutive non-ASCII characters with a space\n",
    "    tweet = re.sub(r'[^\\x00-\\x7F]+',' ', tweet)\n",
    "#to remove the hyperlinks\n",
    "    tweet = re.sub(r'http\\S+','', tweet)\n",
    "#remove emojis from tweet\n",
    "    tweet = emoji_pattern.sub(r'', tweet)\n",
    "#filter using NLTK library append it to a string\n",
    "    filtered_tweet = [w for w in word_tokens if not w in stop_words]\n",
    "    filtered_tweet = []\n",
    "#looping through conditions\n",
    "    for w in word_tokens:\n",
    "#check tokens against stop words , emoticons and punctuations\n",
    "        if w not in stop_words and w not in emoticons and w not in string.punctuation:\n",
    "            filtered_tweet.append(w)\n",
    "    return ' '.join(filtered_tweet)\n",
    "    #print(word_tokens)\n",
    "    #print(filtered_sentence)\n",
    "    return tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "xmas_c = clean_tweets(xmas_s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(xmas_c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xmas_c[1:100]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### I can't figure out why my clean function isn't working"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_twitter(docs):\n",
    "# remove punctuation and numbers\n",
    "# I do this before lemmatizing, so things like \"act's\" turn into 'act' instead of 'act s'\n",
    "    print('removing punctuation and digits')\n",
    "    table = str.maketrans({key: None for key in string.punctuation + string.digits})\n",
    "    clean_text = [d.translate(table) for d in docs]\n",
    "    \n",
    "    print('spacy nlp...')\n",
    "    nlp_docs = [nlp(d) for d in clean_text]\n",
    "    \n",
    "    # keep the word if it's a pronoun, otherwise use the lemma\n",
    "    # otherwise spacy substitutes '-PRON-' for pronouns\n",
    "    print('getting lemmas')\n",
    "    lemmatized_docs = [[w.lemma_ if w.lemma_ != '-PRON-'\n",
    "                           else w.lower_\n",
    "                           for w in d]\n",
    "                      for d in nlp_docs]\n",
    "    \n",
    "    # remove stopwords\n",
    "    print('removing stopwords')\n",
    "    stop_words = set(stopwords.words('english') and ['RT', '...', 'ðŸ“£', 'ðŸŽ„', 'Merry Christmas', \"MERRY CHRISTMAS\"])\n",
    "    \n",
    "    lemmatized_docs = [[lemma for lemma in doc if lemma not in stop_words] for doc in lemmatized_docs]\n",
    "    \n",
    "    # join tokens back into doc\n",
    "    clean_text = [' '.join(l) for l in lemmatized_docs]\n",
    "        \n",
    "    return clean_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = clean_twitter([xmas_s])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#no bueno\n",
    "X[1:1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table = str.maketrans({key: None for key in string.punctuation + string.digits})\n",
    "clean_text = [d.translate(table) for d in xmas_s]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp_docs = [nlp(d) for d in clean_text]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatized_docs = [[w.lemma_ if w.lemma_ != '-PRON-'\n",
    "                    else w.lower_\n",
    "                    for w in d] \n",
    "                   for d in nlp_docs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('english') and \n",
    "                 ['RT', '...', 'ðŸ“£', 'ðŸŽ„', 'Merry Christmas', \"MERRY CHRISTMAS\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatized_docs2 = [[lemma for lemma in doc if lemma not in stop_words] for doc in lemmatized_docs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_text2 = [' '.join(l) for l in lemmatized_docs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##This is where the breakdown is.\n",
    "clean_text[1:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp_docs[1:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatized_docs2[1:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Going simple and will do it manually"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://www.tutorialspoint.com/python/string_maketrans.htm\n",
    "#https://stackoverflow.com/questions/265960/best-way-to-strip-punctuation-from-a-string\n",
    "noP = xmas_s.translate(str.maketrans('', '', string.punctuation))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "noP[0:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = re.sub(r':', '', xmas_s)\n",
    "t1 = re.sub(r'â€šÃ„Â¶', '', t)\n",
    "#replace consecutive non-ASCII characters with a space\n",
    "t2 = re.sub(r'[^\\x00-\\x7F]+',' ', t1)\n",
    "#to remove the hyperlinks\n",
    "t3 = re.sub(r'http\\S+','', t2)\n",
    "#remove emojis from tweet\n",
    "t4 = emoji_pattern.sub(r'', t3)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " t4[0:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "noP = t4.translate(str.maketrans('', '', string.punctuation))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "noP[0:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "noPl = noP.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "noPl[0:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords \n",
    "stop_words = set(stopwords.words('english') and ['rt', 'merry', 'christmas', 'from', 'to', 'the', 'in', \n",
    "                                                 'a', 'of', 'for', 'and', 'it', 's', 'is', 'got', 'an', \n",
    "                                                 'i', 'am', 'so', 't', 'm', 'you', 'u', 'l', 'w', 'at', 'my'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://www.geeksforgeeks.org/removing-stop-words-nltk-python/ \n",
    "#https://www.nltk.org/api/nltk.tokenize.html\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "tokenZR = TweetTokenizer()\n",
    "word_tokens = tokenZR.tokenize(noPl) \n",
    "filtered_sentence = [w for w in word_tokens if not w in stop_words] \n",
    "  \n",
    "filtered_sentence = [] \n",
    "  \n",
    "for w in word_tokens: \n",
    "    if w not in stop_words: \n",
    "        filtered_sentence.append(w) \n",
    "  \n",
    "print(filtered_sentence) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(filtered_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "from nltk.sentiment import vader\n",
    "nltk.download('vader_lexicon')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_analyzer = SentimentIntensityAnalyzer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_analyzer.polarity_scores(filtered_sentence[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_analyzer.polarity_scores(filtered_sentence[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import textblob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = ' '.join([str(elem) for elem in filtered_sentence]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tb = textblob.TextBlob(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tb.sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tb.sentiment_assessments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point I wanted to break a part this data ( textblob.en.sentiments.Sentiment)into a dataframe so that I could work with each typoe of data.  I tried all sorts of ways to break a part the tuples:\n",
    "    creating lists : s = [tb.sentiment_assessments]\n",
    "    lists of lists : l = [list(x) for x in tb.sentiment_assessments]\n",
    "    making the df : \n",
    "        columns=['word', 'polarity', 'subjectivity'] \n",
    "        df = pd.DataFrame([x for x in s], columns=columns, rows = [0])\n",
    "The output was always wonky.  \n",
    "\n",
    "The only other way I could think of doing it would be to creat each measure and then build the df from there."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://towardsdatascience.com/having-fun-with-textblob-7e9eed783d3f\n",
    "import matplotlib.pyplot as plt\n",
    "num_bins = 50\n",
    "plt.figure(figsize=(10,6))\n",
    "n, bins, patches = plt.hist(tb.polarity, num_bins, facecolor='blue', alpha=0.5)\n",
    "plt.xlabel('Polarity')\n",
    "plt.ylabel('Count')\n",
    "plt.title('Histogram of polarity')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because I could not pull the tuples a part I did not create a very informative plot. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The nltk sentiment analysis concluded my Merry Christmas sample was neutral.  One would think the sample would be much more positive.  I did remove as many instances of Merry Christmas as I could so that would not bias the sample.  The textblob analysis yielded much a little more informative conclusions.  The results:  Sentiment(polarity=0.12459997704315885, subjectivity=0.5403681293454021) mean the overall words were positive, which begs the question of what the cut off between neutral and positive in the NLTK analyser is. Also of not the sample was pretty neutral.  These results though are based on what I included in my stopwords.  \n",
    "\n",
    "['rt', 'merry', 'christmas', 'from', 'to', 'the', 'in', 'a', 'of', 'for', 'and', 'it', 's', 'is', 'got',\n",
    "'an', 'i', 'am', 'so', 't', 'm', 'you', 'u', 'l', 'w', 'at', 'my']\n",
    "\n",
    "Perhaps had I included the Merry Christmas, it would have been more positive."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
