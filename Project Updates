# Project Updates

## 30 April 2020
I have completed the following:
1. Created a Github repo and project 
2. Created search queries based on varying timeframes to accomodate the 100 tweet max limit pull
  - This amounts to 11 requests.  
    - r1 = 2020-01-24 22:11:47 --> 2020-01-20 00:00:25
    - r2 = 2020-01-19 23:54:15 --> 2020-01-17 01:15:46 (maxes out at 100, excludes 1 tweet @2020-01-17 00:46:42)
    - r3 = 2020-01-16 20:09:46 --> 2020-01-10 01:20:13
    - r4 = 2020-01-09 23:56:43 --> 2020-01-09 00:41:03
    - r5 = 2020-01-08 23:38:58 --> 2020-01-08 00:21:55
    - r6 = 2020-01-07 23:50:12 --> 2020-01-07 05:38:46 (Missing 5 hours.  UTC time?)
    - r7 = 2020-01-06 21:13:55 --> 2020-01-01 02:33:54
    - r8 = 2019-12-31 21:14:08 --> 2020-01-08 05:39:23
    - r9 = 2020-01-09 23:56:43 --> 2019-12-29 02:24:35
    - r10 = 2019-12-28 23:36:55 -> 2019-12-28 14:50:43 (Missing 14 hours.  UTC time?)
    - r11 = 2019-12-27 23:55:32 -> 2019-12-27 13:11:10 (Missing 13 hours.  UTC time?)
  - Three requests maxed out, with remaining hours not searched.  In order to gather data from these, 
    I think I would need to figure out how to do the query timespan with at least minutes vice just the date.  
    For brevity, I am omitting this additional step. 
3. Next steps include:
  - saving each data request into a file/variable
  - joining each to create one complete data set
  - wrangle data using pandas to isolate tweet attributes that I am interested in
    - tweet text 
    - geo information
  - Conduct NLP on tweet test
    - removing stop words
    - stemming/lemmatization
  - Conduct sentiment analysis with various methodologies
